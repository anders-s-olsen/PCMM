{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from New_grass import weighted_grassmannian_clustering\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import make_interp_spline\n",
    "import pickle\n",
    "import scipy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:24px;\">Loading the eigenvects and clustering data using grassmann clustering</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# Only dealing with data from sleep_run's for each subject\n",
    "# All data has sample size 429\n",
    "cluster_numbers = [2, 3, 4, 5, 6, 8, 10, 12, 15]\n",
    "number_of_subjects = 33 \n",
    "run_number = [2, 3, 4, 5] # sleep-run-numbers\n",
    "all_data_matrices = {} # lib to hold all the sleep_stage data for each subject\n",
    "valid_data_lib = {} # holds information about each subject and for what sleep stage runs we have data // valid_data_lib['01] - subject 1\n",
    "\n",
    "for subject in range(1, number_of_subjects + 1):\n",
    "    subject = str(subject).zfill(2) # fill 01, 02 and so on\n",
    "\n",
    "    all_data_matrices[subject] = {}\n",
    "    valid_data_lib[subject] = []\n",
    "\n",
    "    for number in run_number:\n",
    "        data_matrix = []\n",
    "        with open(f\"sleep_data/sleep_scores/sub-{subject}-sleep-stage.tsv\", \"r\", newline=\"\", encoding=\"utf-8\") as tsv_file: # needs to do this for every number othervise it does not work\n",
    "            tsv_reader = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "            next(tsv_reader) # skip first row to remove column names\n",
    "            for row in tsv_reader:\n",
    "                if subject != \"01\":\n",
    "                    if row[0] == f'task-sleep_run-{number}': \n",
    "                        data_matrix.append(row)\n",
    "                else:\n",
    "                    if row[1] == f'task-sleep_run-{number}': # We do this to handle the case where the first collumn is not 'task-sleep_run-x' as is the case for subject 1\n",
    "                        data_matrix.append(row)   \n",
    "            if len(data_matrix)>0:        \n",
    "                all_data_matrices[subject][number] = data_matrix\n",
    "                valid_data_lib[subject].append(number)\n",
    "\n",
    "for number in run_number: # removing first collumn in data for subject \"01\" as it contains an extra collumn compared to the rest of the data\n",
    "    try: \n",
    "        all_data_matrices[\"01\"][number] = [row[1:] for row in all_data_matrices[\"01\"][number]]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "for subject in all_data_matrices: # removing subjects with no data for sleep stages at all\n",
    "    numbers_to_remove = []\n",
    "    for number in valid_data_lib[subject]: # removing sleep stages that contain a constant state \n",
    "        if len(np.unique([s[0] for s in np.array(all_data_matrices[subject][number])[:, 2]])) == 1:\n",
    "            numbers_to_remove.append(number)\n",
    "    for number in numbers_to_remove:\n",
    "        valid_data_lib[subject].remove(number)\n",
    "    if valid_data_lib[subject] == []:\n",
    "        valid_data_lib.pop(subject)\n",
    "\n",
    "# removing some subjects manually if they seem to contain to much noise or are awake almost all the time - this will not be caught above\n",
    "valid_data_lib.pop('11')\n",
    "valid_data_lib.pop('23')\n",
    "valid_data_lib.pop('27')\n",
    "valid_data_lib.pop('20')\n",
    "valid_data_lib.pop('03')\n",
    "valid_data_lib.pop('07')\n",
    "\n",
    "valid_data_lib = {key: value for key, value in valid_data_lib.items() if len(value) >= len(run_number)} # finally we remove all subjects where we dont have valid data for all (4) runs\n",
    "\n",
    "print(len(valid_data_lib)) # number of subjects left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22308, 116, 2) 22308\n"
     ]
    }
   ],
   "source": [
    "# colleting all data an using vstack to get correct shape\n",
    "all_data = []\n",
    "\n",
    "for subject in valid_data_lib:\n",
    "    for number in valid_data_lib[subject]:\n",
    "        with h5py.File(f\"sleep_data/eigvecs/sub-{subject}_session-task-sleep_run-{number}_eigvecs.h5\", \"r\") as file:\n",
    "            data = file['eigvecs'][:]\n",
    "            all_data.append(data)\n",
    "\n",
    "# Stack all the data vertically\n",
    "all_data = np.vstack(all_data)\n",
    "\n",
    "print(all_data.shape, 13*429*4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22308, 2) 22308\n"
     ]
    }
   ],
   "source": [
    "#eigenvals\n",
    "eigenvals = []\n",
    "for subject in valid_data_lib:\n",
    "    for number in valid_data_lib[subject]:\n",
    "        with h5py.File(f\"sleep_data/eigvals/sub-{subject}_session-task-sleep_run-{number}_eigvals.h5\", \"r\") as file:\n",
    "            data = file['eigvals'][:]\n",
    "            eigenvals.append(data)\n",
    "eigenvals = np.vstack(eigenvals)\n",
    "\n",
    "print(eigenvals.shape, 13*429*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_grassmannian_clustering(X,X_weights,K,number_of_subjects,max_iter=10000,tol=1e-16):\n",
    "    \"\"\"\"\n",
    "    Weighted grassmannian clustering using the chordal distance function and a SVD-based update rule\n",
    "    \n",
    "    X: size (nxpxq), where n is the number of observations, p is the number of features and q is the subspace dimensionality\n",
    "    X_weights: size (n,q), where n is the number of observations and q is the subspace dimensionality (corresponds to eigenvalues)\n",
    "    K: number of clusters\n",
    "    max_iter: maximum number of iterations\n",
    "    tol: tolerance for convergence\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    n,p,q = X.shape\n",
    "\n",
    "    # initialize cluster centers using a normal distribution projected to the Grassmannian\n",
    "    C = np.random.randn(K,p,q)\n",
    "    C_weights = np.ones((K,q))\n",
    "    for k in range(K):\n",
    "        C[k] = C[k]@scipy.linalg.sqrtm(np.linalg.inv(C[k].T@C[k])) # project onto the Grassmannian\n",
    "\n",
    "    # initialize counters\n",
    "    data_number = 429\n",
    "    sub_obj = []\n",
    "    iter = 0\n",
    "    obj = []\n",
    "    partsum = np.zeros((max_iter,K))\n",
    "    while True:\n",
    "        # \"E-step\" - compute the similarity between each matrix and each cluster center\n",
    "        dis = 1/np.sqrt(2)*(np.sum(X_weights**4)+np.sum(C_weights**4)-2*np.linalg.norm(np.swapaxes((X*X_weights[:,None,:])[:,None],-2,-1)@(C*C_weights[:,None,:])[None],axis=(-2,-1)))\n",
    "        sim = -dis\n",
    "        maxsim = np.max(sim,axis=1) # find the maximum similarity - the sum of this value is the objective function\n",
    "        X_part = np.argmax(sim,axis=1) # assign each point to the cluster with the highest similarity\n",
    "        obj.append(np.sum(maxsim))\n",
    "        part_list = []\n",
    "        for idx in range(number_of_subjects):\n",
    "            part_list.append(np.sum(maxsim[idx*data_number : (idx+1)*data_number]))\n",
    "        sub_obj.append(part_list)\n",
    "\n",
    "        # check for convergence\n",
    "        for k in range(K):\n",
    "            partsum[iter,k] = np.sum(X_part==k)\n",
    "        if iter>0:\n",
    "            if all((partsum[iter-1]-partsum[iter])==0) or iter==max_iter or abs(obj[-1]-obj[-2])<tol:\n",
    "                break\n",
    "        \n",
    "        # \"M-step\" - update the cluster centers\n",
    "        for k in range(K):\n",
    "            idx_k = X_part==k\n",
    "            V = np.reshape(np.swapaxes(X[idx_k]*X_weights[idx_k,None,:],0,1),(p,np.sum(idx_k)*q))\n",
    "            U,S,_ = scipy.sparse.linalg.svds(V,q)\n",
    "            C[k] = U[:,:q]\n",
    "            C_weights[k] = S**2\n",
    "\n",
    "        iter += 1\n",
    "    \n",
    "    return C,obj,X_part,sub_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`A` must not be empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m cluster \u001b[39min\u001b[39;00m tqdm(cluster_numbers):\n\u001b[1;32m      4\u001b[0m     cluster_assignments_objfunc_cent[cluster] \u001b[39m=\u001b[39m {}\n\u001b[0;32m----> 5\u001b[0m     C, obj, part, sub_obj \u001b[39m=\u001b[39m weighted_grassmannian_clustering(all_data, eigenvals, cluster, \u001b[39mlen\u001b[39;49m(valid_data_lib), max_iter\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m     cluster_assignments_objfunc_cent[cluster][\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m C\n\u001b[1;32m      7\u001b[0m     cluster_assignments_objfunc_cent[cluster][\u001b[39m'\u001b[39m\u001b[39mobj\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m obj[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m# tager kun den bedste obj value efter grassmann terminater\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 50\u001b[0m, in \u001b[0;36mweighted_grassmannian_clustering\u001b[0;34m(X, X_weights, K, number_of_subjects, max_iter, tol)\u001b[0m\n\u001b[1;32m     48\u001b[0m idx_k \u001b[39m=\u001b[39m X_part\u001b[39m==\u001b[39mk\n\u001b[1;32m     49\u001b[0m V \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(np\u001b[39m.\u001b[39mswapaxes(X[idx_k]\u001b[39m*\u001b[39mX_weights[idx_k,\u001b[39mNone\u001b[39;00m,:],\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m),(p,np\u001b[39m.\u001b[39msum(idx_k)\u001b[39m*\u001b[39mq))\n\u001b[0;32m---> 50\u001b[0m U,S,_ \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49msparse\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49msvds(V,q)\n\u001b[1;32m     51\u001b[0m C[k] \u001b[39m=\u001b[39m U[:,:q]\n\u001b[1;32m     52\u001b[0m C_weights[k] \u001b[39m=\u001b[39m S\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/_svds.py:252\u001b[0m, in \u001b[0;36msvds\u001b[0;34m(A, k, ncv, tol, which, v0, maxiter, return_singular_vectors, solver, random_state, options)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39mPartial singular value decomposition of a sparse matrix.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m \n\u001b[1;32m    249\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m rs_was_None \u001b[39m=\u001b[39m random_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# avoid changing v0 for arpack/lobpcg\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m args \u001b[39m=\u001b[39m _iv(A, k, ncv, tol, which, v0, maxiter, return_singular_vectors,\n\u001b[1;32m    253\u001b[0m            solver, random_state)\n\u001b[1;32m    254\u001b[0m (A, k, ncv, tol, which, v0, maxiter,\n\u001b[1;32m    255\u001b[0m  return_singular_vectors, solver, random_state) \u001b[39m=\u001b[39m args\n\u001b[1;32m    257\u001b[0m largest \u001b[39m=\u001b[39m (which \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLM\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/_svds.py:43\u001b[0m, in \u001b[0;36m_iv\u001b[0;34m(A, k, ncv, tol, which, v0, maxiter, return_singular, solver, random_state)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mprod(A\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     42\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m`A` must not be empty.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n\u001b[1;32m     45\u001b[0m \u001b[39m# input validation/standardization for `k`\u001b[39;00m\n\u001b[1;32m     46\u001b[0m kmax \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(A\u001b[39m.\u001b[39mshape) \u001b[39mif\u001b[39;00m solver \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpropack\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mmin\u001b[39m(A\u001b[39m.\u001b[39mshape) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: `A` must not be empty."
     ]
    }
   ],
   "source": [
    "cluster_assignments_objfunc_cent = {}\n",
    "\n",
    "for cluster in tqdm(cluster_numbers):\n",
    "    cluster_assignments_objfunc_cent[cluster] = {}\n",
    "    C, obj, part, sub_obj = weighted_grassmannian_clustering(all_data, eigenvals, cluster, len(valid_data_lib), max_iter=500)\n",
    "    cluster_assignments_objfunc_cent[cluster]['C'] = C\n",
    "    cluster_assignments_objfunc_cent[cluster]['obj'] = obj[-1] # tager kun den bedste obj value efter grassmann terminater\n",
    "    cluster_assignments_objfunc_cent[cluster]['part'] = part\n",
    "    cluster_assignments_objfunc_cent[cluster]['obj_sub'] = sub_obj[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to a file\n",
    "with open('cluster_assignments_weighted_objfunc.pkl', 'wb') as f:\n",
    "    pickle.dump(cluster_assignments_objfunc_cent, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:24px;\">Using Objective Function Value as meassure for preformance</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:24px;\">Using NMI as meassure for preformance</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:24px;\">1 out of K encoding of the true labels</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:24px;\">1 out of K encoding of predicted labels using grassmann clustering</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:24px;\">Calculating NMI</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dc8e3e4ff9c8fdbed109d8133de8d8f695e288fe9361851b36cbf17bb1975b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
